#!/usr/bin/env perl
#original code by slashbeast (https://github.com/slashbeast)

use warnings; use strict;
use autodie qw / :all /;
use LWP::UserAgent;
use File::Basename qw/ fileparse basename /;
use HTML::TreeBuilder::XPath -weak;
use LWP::Simple; 
use HTTP::Cookies;
use Data::Dumper;
use Tie::File;
use Time::Piece;

chdir ((fileparse(__FILE__))[1]);

my $name = $ARGV[0];

die "Address?\n" if not $name;

my $fm_is_avalabile = eval {
    require Parallel::ForkManager;
    Parallel::ForkManager->import;
    1
};

sub einfo {
    printf( "[INFO] >>> %s\n", join( ' ', @_ ) );
}

sub fetch {
    my ( $url, $save_to ) = @_;

    my $ua = LWP::UserAgent->new;

    if ( -e 'ids' ) {
        my $cookies = HTTP::Cookies->new({});
        tie my @session_ids, 'Tie::File', 'ids';
        #i think one is enough but just to be safe we set both
        if ($session_ids[0]) { $cookies->set_cookie(0, 'soup_session_id', $session_ids[0], '/', "$ARGV[0]", 80, 0, 0, 86400, 0) }
        if ($session_ids[1]) { $cookies->set_cookie(0, 'soup_session_id', $session_ids[1], '/', 'soup.io', 80, 0, 0, 86400, 0) }
        $ua->cookie_jar($cookies);
    }

    # Because 403 on images.
    $ua->agent("Mozilla/5.0 (X11; Linux x86_64; rv:51.0) Gecko/20100101 Firefox/51.0");

    einfo( "Fetching $url ..." );

    my $response = $ua->get( $url, 'Accept-Encoding' => 'gzip' );

    if ( $response->is_success ) {
        my $content = $response->decoded_content;

        if ( $save_to ) {
            open my $file, '>', $save_to;

            if ( utf8::is_utf8( $content ) ) {
                binmode $file,':utf8';
            } else {
                binmode $file,':raw';
            }

            print $file $content;
            close $file;
            return 1;
        } else {
            return $content;
        }
    } else {
        die "Fetch failed, " . $response->code() . ".\n";
    }
}

sub list_posts {
    my $page = shift;

    my %unique_posts = map { $_ => 1 } ($page =~ m/http[s]?:\/\/$name\/post\/[0-9]+/gi);

    return keys %unique_posts;
}

sub list_images {
    my $post = shift;

    my %unique_images;
    for ($post =~ m/src=[^>]+((http[s]?:\/\/asset-[^.]+\.soup(\.io|cdn\.com)\/asset\/[0-9]+)\/([0-9a-zA-Z]+_[0-9a-zA-Z]+)(_[0-9a-zA-Z]+)?\.([a-zA-Z0-9]+))/g) {
        $unique_images{"$2/$4.$6"} = 1;
    }

    return keys %unique_images;
}

sub get_data {
    my $post_url = shift;

    my $post = fetch $post_url;
    my @images = list_images $post;

    my $html = HTML::TreeBuilder::XPath->new_from_content($post);
    my $date = $html->findnodes('//span[@class="time hidden"]/abbr/@title');
    $date = Time::Piece->strptime($date, '%b %d %Y %T UTC');
    my $repostedfrom = $html->findnodes('//div[@class="source"]');
    my $reactionto = $html->findvalue('//div[@class="toggle"]/a[2]/@href');
    $reactionto = length $reactionto ? "Reaction to $reactionto" : '';
    my $description = $html->findvalue('//div[contains(@class, "content")]/div[contains(@class, "description")]');
    my $source = $html->findnodes('//div[@class="caption"]/a/@href');
    #this instead of non-destructive substitution because it breaks syntax highlighting in my sublime lol
    (my $postid = $html->findnodes('//div[@id="first_batch"]/div[1]/@id')) =~ s/\D//g;
    my $filename = join '_', grep $_, $date->epoch, $postid;
    my $comment = join ' | ', grep $_, $reactionto, $repostedfrom, $description, $source;

    for (0 .. $#images) {
        my ($n, $d, $extension) = fileparse($images[$_], qr/\.[^.]*$/);
        my $image = "$filename\_$_$extension";
        my $target = "archives/$name/$image";
        if ( not -f $target ) {
            fetch $images[$_], "$target.tmp";
            rename "$target.tmp", $target;
        }
        tie my @post_meta, 'Tie::File', "archives/$name/$filename\_$_.txt";
        $post_meta[0] = $date->epoch == 0 ? $postid : $date;
        $post_meta[1] = $comment;
    }
 }

mkdir 'archives' if not -d 'archives';
mkdir "archives/$name" if not -d "archives/$name";

my $link = "http://" . $name;

while( $link ) {
    my $page = fetch $link;

    my @posts = list_posts $page;

    if ($fm_is_avalabile) {
        my $fm = new Parallel::ForkManager(25);
        $fm->set_waitpid_blocking_sleep(0);
        
        for (@posts) {
            $fm->start and next;
            
            get_data($_);

            $fm->finish;
        }
        $fm->wait_all_children;
    } else {
        get_data($_) for @posts;
    }

    if ($page =~ m/SOUP\.Endless\.next_url = .(\/since\/[0-9]+\?mode=own)/) {
        $link = "http://" . $name . $1;
    } else {
        undef $link;
    }
}